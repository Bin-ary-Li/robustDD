# -*- coding: utf-8 -*-
"""DDD_Resnet50.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bksGxJx8swNCwK5OGKtQ8PayeDgjTMwI
"""

import torchvision
import torch
import torchvision.transforms as transforms
from torchvision.models import resnet50
from tqdm import tqdm
import argparse
from sklearn.metrics import accuracy_score
import os

def doArgs():
    parser = argparse.ArgumentParser(description='parameters for Double Decent')

    parser.add_argument('--dataset', type=str, help="Noised Cifar10 dataset path, type Cifar10 to use original dataset", required=True)
    parser.add_argument('--cls', type=int, help="num of classes", required=True)
    parser.add_argument('--width', type=int, help="width of model", required=True)
    return parser.parse_args()

# model hyperparameter 
def main():
    args = doArgs()

    width = args.width

    n_cls = args.cls

    dataset = args.dataset

    mod = resnet50(width_per_group=width)
    mod.fc = torch.nn.Linear(2048, n_cls)

    transform = transforms.Compose(
        [transforms.RandomCrop(32,padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

    batch_size = 256

    if (dataset == 'Cifar10'):
        trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                                download=True, transform=transform)
    else:
        trainset = torch.load(dataset)


    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,
                                        shuffle=True, num_workers=2)

    testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                    download=True, transform=transform)
    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,
                                        shuffle=False, num_workers=2)
                
    lr = 0.0001
    criterion = torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(mod.parameters(), lr=lr)

    mod = mod.cuda()
    for epoch in range(1000):  # loop over the dataset multiple times

        # running_loss = 0.0
        # for i, data in enumerate(trainloader, 0):
            # get the inputs; data is a list of [inputs, labels]
            # inputs, labels = data
        with tqdm(trainloader, unit="batch") as tepoch:
            for inputs, labels in tepoch:
                tepoch.set_description(f'Width {width} | Epoch {epoch}')
                
                inputs = inputs.cuda()
                labels = labels.cuda()

                # zero the parameter gradients
                optimizer.zero_grad()

                # forward + backward + optimize
                outputs = mod(inputs)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()

                # # print statistics
                # running_loss += loss.item()
                tepoch.set_postfix(loss=loss.item())
            # print(f'[{epoch + 1}] loss: {running_loss / len(trainloader):.3f}')

    print('Finished Training in Dataset {} with width {}'.format(dataset, width))

    correct = 0
    total = 0
    # since we're not training, we don't need to calculate the gradients for our outputs
    with torch.no_grad():
        for data in trainloader:
            images, labels = data
            # calculate outputs by running images through the network
            outputs = mod(images.cuda())
            # the class with the highest energy is what we choose as prediction
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted.cpu() == labels).sum().item()
    trainerr = 1 - correct / total

    correct = 0
    total = 0
    # since we're not training, we don't need to calculate the gradients for our outputs
    with torch.no_grad():
        for data in testloader:
            images, labels = data
            # calculate outputs by running images through the network
            outputs = mod(images.cuda())
            # the class with the highest energy is what we choose as prediction
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted.cpu() == labels).sum().item()
    testerr = 1 - correct / total
    
    mod = mod.cpu()
    print('Dataset {} with width {} has train error {} and test error {}'.format(dataset, width, trainerr, testerr))

    f = open('Dataset {} width {} trainerr {} testerr {}'.format(dataset, width, trainerr, testerr), 'w')
    f.close()


if __name__ == '__main__':
    main()
